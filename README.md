# Spring2024
Repository for the Spring 2024 Computational Social Science Workshop

Time: 9:30 AM to 11:50 PM, Thursdays
Location: 1155 E. 60th Street, Chicago IL 60637; Room 295

Sign up to meet over lunch, dinner, or small group settings with our speakers [**here**](https://docs.google.com/spreadsheets/d/18tySJ3FJ8zT8Sh8JW6SixkFsZCOMSZOqru8kVEzVQwo/edit?usp=sharing)

**Future seminars**: [Xuechunzi Bai](https://www.xuechunzibai.com/) (5/2, UChicago CogSci & Psychology, in-person),	[Grant Blank](https://www.oii.ox.ac.uk/people/profiles/grant-blank/) (5/9, Oxford Internet Institute, in-person), and [Jake Hoffman](https://www.microsoft.com/en-us/research/people/jmh/) (5/16, Microsoft Research, in-person)!

**4/25** [Uri Hasson](https://psychology.princeton.edu/people/uri-hasson) is Professor of Psychology at the Princeton University who studies the neural basis of brain-to-brain human communication, natural language processing, and language acquisition. He aims to develop new theoretical frameworks and computational tools to model the neural basis of cognition as it materializes in the real world, inspired by the success of deep learning in modeling natural stimuli. He and his team are searching for shared computational principles and inherent differences in how the brain and deep neural networks process natural language, with findings that suggest how deep language models provide a new computational framework for studying the neural basis of language.

**Deep language models as a cognitive model for natural language processing in the human brain**. Naturalistic experimental paradigms in cognitive neuroscience arose from a pressure to test, in real-world contexts, the validity of models we derive from highly controlled laboratory experiments. In many cases, however, such efforts led to the realization that models (i.e., explanatory principles) developed under particular experimental manipulations fail to capture many aspects of reality (variance) in the real world. Recent advances in artificial neural networks provide an alternative computational framework for modeling cognition in natural contexts. In this talk, I will ask whether the human brain's underlying computations are similar or different from the underlying computations in deep neural networks, focusing on the underlying neural process that supports natural language processing in adults and language development in children. I will provide evidence for some shared computational principles between deep language models and the neural code for natural language processing in the human brain. This indicates that, to some extent, the brain relies on overparameterized optimization methods to comprehend and produce language. At the same time, I will present evidence that the brain differs from deep language models as speakers try to convey new ideas and thoughts. Finally, I will discuss our ongoing attempt to use deep acoustic-to-speech-to-language models to model language acquisition in children.

Pose your questions [here](https://github.com/uchicago-computation-workshop/Spring2024/issues/4)!

**4/18** [Nilam Ram](https://profiles.stanford.edu/nilam-ram) is a professor of Communications at Stanford University who studies the dynamic interplay of psychological and media processes and how they change moment-to-moment and across the life span. This workshop is **VIRTUAL** only. online. 

**Modeling at Multiple Time-Scales: Screenomics and Other Super-Intensive Longitudinal Paradigms**. A decade ago, we used newly emerging smartphone technologies to obtain multiple time-scale data that facilitated study of new intraindividual variability constructs and how they changed over time. The recent merging of daily and digital life further opens opportunity to observe, probe, and modify every imaginable aspect of human behavior – at a scale we never imagined. Using collections of intensive longitudinal data from survey panels, experience sampling studies, social media, laboratory observations, and our new Screenomics paradigm, I illustrate how methodological invocation of zooms, tensions, and switches (ZOOTS) is transforming our understanding of human dynamics and development. Along the way, I develop calls for more flexible definitions of time, fluidity and diversity of methodological approach, and engagement with science that adds good into the world. Two short, related papers available [here](https://github.com/uchicago-computation-workshop/Spring2024/files/15014884/Reeves_etal_Nature_2020.pdf) and [here](https://github.com/uchicago-computation-workshop/Spring2024/files/15014982/Ram_etal_MolenaarFestchrift_MBR_2023.pdf).

Pose your questions [here](https://github.com/uchicago-computation-workshop/Spring2024/issues/3)!

**4/11** [Ashton Anderson](https://www.cs.toronto.edu/~ashton/) is an Associate Professor in the Department of Computer Science at the University of Toronto, broadly interested in the intersection of AI, data, and society. He runs the [Computational Social Science Lab](https://csslab.cs.toronto.edu/) at the University of Toronto and has made major advances to large-scale understanding of online communities, polarization, and AI designed to collaborate with humans.

**Generative AI for Human Benefit: Lessons from Chess**. Artificial intelligence is becoming increasingly intelligent, kicking off a Cambrian explosion of AI models filling thousands of niches. Although these tools may replace human effort in some domains, many other areas will foster a combination of human and AI participation. A central challenge in realizing the full potential of human-AI collaboration is that algorithms often act very differently than people, and thus may be uninterpretable, hard to learn from, or even dangerous for humans to follow. For the past six years, my group has been exploring how to align generative AI for human benefit in an ideal model system, chess, in which AI has been superhuman for over two decades, a massive amount of fine-grained data on human actions is available, and a wide spectrum of skill levels exist. We developed Maia, a generative AI model that captures human style and ability in chess across the spectrum of human skill, and predicts likely next human actions analogously to how large language models predict likely next tokens. The Maia project started with these aggregated population models, which have now played millions of games against human opponents online, and has grown to encompass individual models that act like specific people, embedding models that can identify a person by a small sample of their actions alone, an ethical framework for issues that arise with individual models in any domain, various types of partner agents designed from combining human-like and superhuman AI, and algorithmic teaching systems. In this talk, I will share our approaches to designing generative AI for human benefit and the broadly applicable lessons we have learned about human-AI interaction. Paper: ["Designing Skill-Compatible AI: Methodologies and Frameworks in Chess", Karim Hamade, Reid McIlroy-Young, Siddhartha Sen, Jon Kleinberg, and Ashton Anderson.](https://urldefense.com/v3/__https://www.cs.toronto.edu/*ashton/pubs/maia-partner-iclr24.pdf__;fg!!BpyFHLRN4TMTrA!5ggUKNc5qX0BM3wzdSdqoyryrug_ikaRob9TYpuygrh-tiCNwSPtgekQKn8D86xZbcdCn85Ro5otlccayN9yQIw$) in ICLR 2024.

Pose your questions [here](https://github.com/uchicago-computation-workshop/Spring2024/issues/2)!

**3/28** [John Wixted](https://psychology.ucsd.edu/people/profiles/jwixted.html), is a Distinguished Professor of Psychology at UCSD with research focused on understanding episodic memory. His work investigates the cognitive mechanisms that underlie recognition memory, often drawing upon signal detection theory. He also investigates how episodic memory is represented in the human hippocampus, based mainly on single-unit recording studies performed with epilepsy patients. In recent years, his research has also investigated the applied implications of signal detection-based models of recognition memory and its implications for the reliability of eyewitness memory.

**Emerging Insights into the Reliability of Eyewitness Memory**. Eyewitness misidentifications have contributed to many wrongful convictions. However, despite expressing high confidence at trial, eyewitnesses often make inconclusive misidentifications on the first test conducted early in a police investigation. According to a new scientific consensus, it is important to focus on the results of the first test because, if the perpetrator is not in the lineup, the test itself leaves a memory trace of the innocent suspect in the witness’s brain. Thus, all subsequent tests of the witness’s memory for the same suspect constitute tests of contaminated memory. Unfortunately, when evidence of an initial inconclusive identification is introduced at trial, the rules of evidence provide a witness with an opportunity to explain their prior inconsistent statement. In response, witnesses often provide an opinion about why they did not confidently identify the suspect on the initial test despite doing so now (e.g., “I was nervous on the first test”). However, witnesses lack expertise in—and have no awareness of—the subconscious mechanisms of memory contamination that have been elucidated by decades of scientific research. The combination of a sincerely held (false) memory and a believable (but erroneous) explanation for a prior inconsistent statement is often persuasive to jurors. This is a recipe for a wrongful conviction, one that has been followed many times. These wrongful convictions, which have long been attributed to the unreliability of eyewitness memory, instead reflect a system that unwittingly prioritizes false memories elicited at trial over true memories elicited early in a police investigation. The Federal Rules of Evidence were enacted almost a half-century ago, and it may be time to revisit them in light of the principles of memory that have been established since that time. Related paper on **The Mechanisms of Memory vs. the Federal Rules of Evidence** sent by email.

Pose your questions [**here!**](https://github.com/uchicago-computation-workshop/Spring2024/issues/1)
